An implementation of a Quantized Transformer Model. Based on the work of ["Attention is All You Need" (Vaswani et al. 2017)](https://arxiv.org/pdf/1706.03762.pdf) and ["Quantized Neural Networks: Training Neural Networks with
Low Precision Weights and Activations" (Hubara et al. 2018)](http://jmlr.org/papers/volume18/16-456/16-456.pdf).
